{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9656a8",
   "metadata": {},
   "source": [
    "# TP1 Discussion and Analysis of Word Embeddings\n",
    " \n",
    "**Lab:** TP1 Word Embeddings Training  \n",
    "**Student:** Ilyes sais, Wenchong PAN, Muhammed Qaisar \n",
    "**Year:** 2025–2026  \n",
    "\n",
    "This notebook provides a detailed discussion and qualitative analysis of the word embeddings trained in TP1.  \n",
    "We compare different embedding approaches (Word2Vec CBOW, Word2Vec Skip-gram, FastText CBOW) trained on two corpora:\n",
    "\n",
    "- **Medical domain corpus (QUAERO_FrenchMed)**\n",
    "- **General press corpus (QUAERO_FrenchPress)**\n",
    "\n",
    "The analysis is based on semantic similarity results obtained in Notebook 06.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff7f21",
   "metadata": {},
   "source": [
    "## 1. Experimental Setup Recap\n",
    "\n",
    "In TP1, we trained six word embedding models using the following configurations:\n",
    "\n",
    "### Embedding Models\n",
    "- **Word2Vec CBOW**\n",
    "- **Word2Vec Skip-gram**\n",
    "- **FastText CBOW**\n",
    "\n",
    "### Training Corpora\n",
    "- **Medical corpus:** QUAERO_FrenchMed (≈ 3,021 sentences)\n",
    "- **Press corpus:** QUAERO_FrenchPress (≈ 38,548 sentences)\n",
    "\n",
    "### Shared Hyperparameters\n",
    "- Embedding dimension: **100**\n",
    "- Window size: **5**\n",
    "- Minimum word count: **1**\n",
    "- Epochs: **10**\n",
    "\n",
    "The embeddings were evaluated qualitatively using cosine similarity on the following target words:\n",
    "\n",
    "> **patient, traitement, maladie, solution, jaune**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f4cc3",
   "metadata": {},
   "source": [
    "## 2. Word2Vec Models on the Medical Corpus\n",
    "\n",
    "### 2.1 CBOW Medical Corpus\n",
    "\n",
    "The Word2Vec CBOW model trained on the medical corpus shows strong domain-specific behavior.\n",
    "\n",
    "Examples:\n",
    "- **\"traitement\"** → *médecin, TYSABRI, devra*\n",
    "- **\"solution\"** → *poudre, flacon, diluer*\n",
    "- **\"maladie\"** → *administration, évolution*\n",
    "\n",
    "These neighbors are clearly related to:\n",
    "- medical procedures,\n",
    "- pharmaceutical products,\n",
    "- treatment instructions.\n",
    "\n",
    "However, some highly frequent function words (e.g. *que, la, du*) also appear, which indicates that CBOW is sensitive to high-frequency context words in small corpora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5ba2b",
   "metadata": {},
   "source": [
    "### 2.2 Skip-gram Medical Corpus\n",
    "\n",
    "The Skip-gram model provides more semantically focused representations.\n",
    "\n",
    "Notable observations:\n",
    "- **\"traitement\"** → *cancer, Parkinson, expérimenté*\n",
    "- **\"solution\"** → *injectable, perfusion, intraveineuse*\n",
    "- **\"maladie\"** → *Parkinson, charge, liée*\n",
    "\n",
    "Compared to CBOW:\n",
    "- Skip-gram captures **rarer but more informative medical terms**\n",
    "- It better models **specialized medical vocabulary**\n",
    "\n",
    "This behavior is expected, as Skip-gram is known to perform better on infrequent words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c956853",
   "metadata": {},
   "source": [
    "## 3. Word2Vec Models on the Press Corpus\n",
    "\n",
    "### 3.1 CBOW Press Corpus\n",
    "\n",
    "On the press corpus, semantic similarity is more general and abstract.\n",
    "\n",
    "Examples:\n",
    "- **\"traitement\"** → *coût, financement, système*\n",
    "- **\"solution\"** → *recette, règle, alternative*\n",
    "- **\"maladie\"** → *population, mondialisation*\n",
    "\n",
    "This reflects:\n",
    "- political,\n",
    "- economic,\n",
    "- societal discourse.\n",
    "\n",
    "Medical meaning is largely diluted due to the general-domain nature of the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55680588",
   "metadata": {},
   "source": [
    "### 3.2 Skip-gram Press Corpus\n",
    "\n",
    "Skip-gram on the press corpus still improves semantic precision compared to CBOW.\n",
    "\n",
    "Examples:\n",
    "- **\"maladie\"** → *Alzheimer, grippe, pneumopathie*\n",
    "- **\"solution\"** → *pacifique, consensuelle, vitale*\n",
    "\n",
    "Nevertheless, the representations remain less specialized than those obtained from the medical corpus.\n",
    "\n",
    "This highlights the strong influence of **training data domain** on embedding quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc61cf5",
   "metadata": {},
   "source": [
    "## 4. FastText CBOW Medical Corpus\n",
    "\n",
    "FastText shows very strong performance on the medical corpus.\n",
    "\n",
    "Key observations:\n",
    "- Near-perfect similarity scores (≈ 1.0)\n",
    "- Strong morphological awareness\n",
    "\n",
    "Examples:\n",
    "- **\"traitement\"** → *Traitement, traitements, Allaitement*\n",
    "- **\"patient\"** → *patiente, Patient*\n",
    "- **\"solution\"** → *dilution, dissolution*\n",
    "\n",
    "FastText benefits from:\n",
    "- character n-grams,\n",
    "- robustness to capitalization,\n",
    "- handling of rare or misspelled words.\n",
    "\n",
    "This makes it particularly well-suited for medical texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539af0f",
   "metadata": {},
   "source": [
    "## 5. FastText CBOW Press Corpus\n",
    "\n",
    "On the press corpus, FastText mainly captures **morphological similarity** rather than semantic specialization.\n",
    "\n",
    "Examples:\n",
    "- **\"patient\"** → *impatient, patientent*\n",
    "- **\"solution\"** → *révolution, résolution*\n",
    "- **\"jaune\"** → *lune, brune, Jeune*\n",
    "\n",
    "While linguistically valid, these neighbors are:\n",
    "- less semantically meaningful for NER,\n",
    "- often based on suffix or character overlap.\n",
    "\n",
    "This confirms that FastText excels at form-level similarity, especially in large general corpora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a819bd",
   "metadata": {},
   "source": [
    "## 6. Global Comparison of Embedding Approaches\n",
    "\n",
    "| Model | Medical Corpus | Press Corpus |\n",
    "|------|---------------|--------------|\n",
    "| Word2Vec CBOW | Good contextual coverage, but noisy | General, abstract semantics |\n",
    "| Word2Vec Skip-gram | Strong medical semantics | Better than CBOW |\n",
    "| FastText CBOW | Excellent domain + morphology | Mainly morphological similarity |\n",
    "\n",
    "### Key Findings\n",
    "- **Domain matters more than model choice**\n",
    "- Skip-gram outperforms CBOW for specialized vocabulary\n",
    "- FastText is best for rare words and morphology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a685e58",
   "metadata": {},
   "source": [
    "## 7. Implications for Named Entity Recognition (TP2)\n",
    "\n",
    "Based on these observations, we expect:\n",
    "\n",
    "- **Medical embeddings > Press embeddings** for NER in the medical domain\n",
    "- **FastText medical embeddings** to perform best for:\n",
    "  - rare entities,\n",
    "  - complex terminology,\n",
    "  - spelling variations\n",
    "- **Skip-gram medical embeddings** to provide strong semantic consistency\n",
    "\n",
    "These hypotheses will be validated quantitatively in TP2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f0bc8",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This qualitative evaluation demonstrates that:\n",
    "\n",
    "- Word embeddings strongly depend on the **training corpus**\n",
    "- Medical-domain embeddings capture precise biomedical semantics\n",
    "- FastText offers superior robustness through subword modeling\n",
    "- Skip-gram is particularly effective for rare and technical terms\n",
    "\n",
    "Overall, **FastText and Skip-gram trained on medical data** are the most promising candidates for the downstream NER task in TP2.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
