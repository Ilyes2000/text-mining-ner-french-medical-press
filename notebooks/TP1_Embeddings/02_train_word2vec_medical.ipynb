{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b12e65f",
   "metadata": {},
   "source": [
    "# TP1 Word Embeddings  \n",
    "## Training Word2Vec on the Medical Corpus (QUAERO_FrenchMed)\n",
    "\n",
    "In this notebook, we train **Word2Vec embeddings** on the **medical corpus**\n",
    "(QUAERO_FrenchMed).\n",
    "\n",
    "We follow the instructions of the lab:\n",
    "- Library: `gensim`\n",
    "- Models: **CBOW** and **Skip-gram**\n",
    "- Embedding dimension: **100**\n",
    "- min_count: **1**\n",
    "\n",
    "These embeddings will later be:\n",
    "- qualitatively evaluated using semantic similarity\n",
    "- reused in TP2 for Named Entity Recognition \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f570c34",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e047fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4394d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical corpus path: /Users/ilyessais/Documents/Lab M2/Text Mining & Chatbots/text-mining-chatbots-lab3/TP_ISD2020/data/embeddings_corpus/QUAERO_FrenchMed/QUAERO_FrenchMed_traindev.ospl\n",
      "Embeddings will be saved in: /Users/ilyessais/Documents/Lab M2/Text Mining & Chatbots/text-mining-chatbots-lab3/TP_ISD2020/embeddings\n"
     ]
    }
   ],
   "source": [
    "CURRENT_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, \"../..\"))\n",
    "\n",
    "# Corpus path\n",
    "MEDICAL_CORPUS_PATH = os.path.join(\n",
    "    PROJECT_ROOT,\n",
    "    \"data\",\n",
    "    \"embeddings_corpus\",\n",
    "    \"QUAERO_FrenchMed\",\n",
    "    \"QUAERO_FrenchMed_traindev.ospl\"\n",
    ")\n",
    "\n",
    "# Output directory for embeddings\n",
    "EMBEDDINGS_DIR = os.path.join(PROJECT_ROOT, \"embeddings\")\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Medical corpus path:\", MEDICAL_CORPUS_PATH)\n",
    "print(\"Embeddings will be saved in:\", EMBEDDINGS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77d36a",
   "metadata": {},
   "source": [
    "## Load and Tokenize the Medical Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1ed323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of medical sentences: 3021\n",
      "Example tokenized sentence:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['EMEA', '/', 'H', '/', 'C', '/', '551']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tokenized_corpus(path):\n",
    "    \"\"\"\n",
    "    Load a corpus in OSPL format:  one sentence per line tokens separated by spaces\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                tokens = line.split()\n",
    "                sentences.append(tokens)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "medical_sentences = load_tokenized_corpus(MEDICAL_CORPUS_PATH)\n",
    "\n",
    "print(f\"Number of medical sentences: {len(medical_sentences)}\")\n",
    "print(\"Example tokenized sentence:\")\n",
    "medical_sentences[0][:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9042c",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1656db75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 100\n",
      "Min count: 1\n",
      "Window size: 5\n",
      "Epochs: 10\n",
      "Workers: 12\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "EMBEDDING_DIM = 100\n",
    "MIN_COUNT = 1\n",
    "WINDOW_SIZE = 5\n",
    "EPOCHS = 10\n",
    "\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Embedding dimension:\", EMBEDDING_DIM)\n",
    "print(\"Min count:\", MIN_COUNT)\n",
    "print(\"Window size:\", WINDOW_SIZE)\n",
    "print(\"Epochs:\", EPOCHS)\n",
    "print(\"Workers:\", N_WORKERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e3ea91",
   "metadata": {},
   "source": [
    "## Train Word2Vec CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52546e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec CBOW model on medical corpus...\n",
      "CBOW training completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Word2Vec CBOW model on medical corpus...\")\n",
    "\n",
    "w2v_medical_cbow = Word2Vec(\n",
    "    sentences=medical_sentences,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=WINDOW_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    sg=0,          \n",
    "    workers=N_WORKERS,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"CBOW training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb36755",
   "metadata": {},
   "source": [
    "## Save CBOW Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88ffa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW model saved to: /Users/ilyessais/Documents/Lab M2/Text Mining & Chatbots/text-mining-chatbots-lab3/TP_ISD2020/embeddings/word2vec_medical_cbow.model\n",
      "CBOW vectors saved to: /Users/ilyessais/Documents/Lab M2/Text Mining & Chatbots/text-mining-chatbots-lab3/TP_ISD2020/embeddings/word2vec_medical_cbow.vec\n"
     ]
    }
   ],
   "source": [
    "cbow_model_path = os.path.join(EMBEDDINGS_DIR, \"word2vec_medical_cbow.model\")\n",
    "cbow_vectors_path = os.path.join(EMBEDDINGS_DIR, \"word2vec_medical_cbow.vec\")\n",
    "\n",
    "w2v_medical_cbow.save(cbow_model_path)\n",
    "w2v_medical_cbow.wv.save_word2vec_format(cbow_vectors_path)\n",
    "\n",
    "print(\"CBOW model saved to:\", cbow_model_path)\n",
    "print(\"CBOW vectors saved to:\", cbow_vectors_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00214362",
   "metadata": {},
   "source": [
    "## Train Word2Vec Skip-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e76f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Skip-gram model on medical corpus...\n",
      "Skip-gram training completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Word2Vec Skip-gram model on medical corpus...\")\n",
    "\n",
    "w2v_medical_sg = Word2Vec(\n",
    "    sentences=medical_sentences,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=WINDOW_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    sg=1,           \n",
    "    workers=N_WORKERS,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"Skip-gram training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71905dc5",
   "metadata": {},
   "source": [
    "## Save Skip-gram Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76f1dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram model saved to: /Users/ilyessais/Documents/Lab M2/Text Mining & Chatbots/text-mining-chatbots-lab3/TP_ISD2020/embeddings/word2vec_medical_skipgram.model\n",
      "Skip-gram vectors saved to: /Users/ilyessais/Documents/Lab M2/Text Mining & Chatbots/text-mining-chatbots-lab3/TP_ISD2020/embeddings/word2vec_medical_skipgram.vec\n"
     ]
    }
   ],
   "source": [
    "sg_model_path = os.path.join(EMBEDDINGS_DIR, \"word2vec_medical_skipgram.model\")\n",
    "sg_vectors_path = os.path.join(EMBEDDINGS_DIR, \"word2vec_medical_skipgram.vec\")\n",
    "\n",
    "w2v_medical_sg.save(sg_model_path)\n",
    "w2v_medical_sg.wv.save_word2vec_format(sg_vectors_path)\n",
    "\n",
    "print(\"Skip-gram model saved to:\", sg_model_path)\n",
    "print(\"Skip-gram vectors saved to:\", sg_vectors_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9b4c2",
   "metadata": {},
   "source": [
    "## Quick Sanity Check: Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7488685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW vocabulary size: 9104\n",
      "Skip-gram vocabulary size: 9104\n"
     ]
    }
   ],
   "source": [
    "print(\"CBOW vocabulary size:\", len(w2v_medical_cbow.wv))\n",
    "print(\"Skip-gram vocabulary size:\", len(w2v_medical_sg.wv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b6a81",
   "metadata": {},
   "source": [
    "## Quick Semantic Check (Medical Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e70ee38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words to 'patient' (CBOW):\n",
      "  cette           0.999\n",
      "  Le              0.999\n",
      "  produit         0.999\n",
      "  plus            0.999\n",
      "  qui             0.999\n",
      "\n",
      "Most similar words to 'traitement' (CBOW):\n",
      "  que             0.995\n",
      "  devra           0.995\n",
      "  médecin         0.995\n",
      "  TYSABRI         0.994\n",
      "  qu              0.993\n",
      "\n",
      "Most similar words to 'maladie' (CBOW):\n",
      "  évolution       0.998\n",
      "  du              0.998\n",
      "  administration  0.998\n",
      "  souris          0.998\n",
      "  la              0.998\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"patient\", \"traitement\", \"maladie\"]\n",
    "\n",
    "for word in test_words:\n",
    "    if word in w2v_medical_cbow.wv:\n",
    "        print(f\"\\nMost similar words to '{word}' (CBOW):\")\n",
    "        for w, s in w2v_medical_cbow.wv.most_similar(word, topn=5):\n",
    "            print(f\"  {w:<15} {s:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nWord '{word}' not found in CBOW vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f72855",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we successfully trained two **Word2Vec models**\n",
    "on the **medical corpus (QUAERO_FrenchMed)**:\n",
    "\n",
    "- Word2Vec **CBOW**\n",
    "- Word2Vec **Skip-gram**\n",
    "\n",
    "Both models:\n",
    "- use 100 dimensional embeddings\n",
    "- include all words (min_count = 1)\n",
    "- capture domain-specific medical vocabulary\n",
    "\n",
    "These embeddings will be:\n",
    "- compared with press-domain embeddings\n",
    "- evaluated using semantic similarity\n",
    "- reused as pretrained embeddings in TP2 for NER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2866904",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
