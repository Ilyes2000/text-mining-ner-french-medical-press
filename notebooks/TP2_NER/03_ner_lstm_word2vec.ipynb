{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f17f85",
   "metadata": {},
   "source": [
    "# TP2 Named Entity Recognition\n",
    "## LSTM with pretrained Word2Vec embeddings\n",
    "\n",
    "In this notebook, we train a Named Entity Recognition (NER) model using an LSTM\n",
    "architecture initialized with **pretrained Word2Vec embeddings** learned during TP1.\n",
    "\n",
    "The objective is to evaluate the impact of domain specific pretrained embeddings\n",
    "compared to random embeddings on the NER task.\n",
    "\n",
    "We strictly rely on the LSTM/CNN script provided by the instructor and only adapt\n",
    "the embedding initialization step.\n",
    "\n",
    "In this notebook, we train a Named Entity Recognition model using the **official\n",
    "cnn_classification.py script provided by the instructor**.\n",
    "\n",
    "The script is executed as an external program and **is not modified**.\n",
    "All experiments are launched by passing arguments exactly as expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e67cfb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a7dffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec, KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "652c15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/ner_processed/final\"\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"emea_train.csv\")\n",
    "DEV_PATH   = os.path.join(DATA_DIR, \"emea_dev.csv\")\n",
    "TEST_PATH  = os.path.join(DATA_DIR, \"emea_test.csv\")\n",
    "\n",
    "# Word2Vec model trained in TP1 (medical corpus)\n",
    "W2V_MODEL_PATH = \"../../embeddings/word2vec_medical_cbow.model\"\n",
    "\n",
    "EMBEDDING_DIM = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa272bb",
   "metadata": {},
   "source": [
    "## Load NER data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15eafe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (706, 2)\n",
      "Dev size: (649, 2)\n",
      "Test size: (578, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRIALT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EMEA / H / C / 551</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qu ’ est ce que Prialt ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prialt est une solution pour perfusion contena...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dans quel cas Prialt est - il utilisé ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0                                             PRIALT      1\n",
       "1                                 EMEA / H / C / 551      0\n",
       "2                           Qu ’ est ce que Prialt ?      1\n",
       "3  Prialt est une solution pour perfusion contena...      1\n",
       "4            Dans quel cas Prialt est - il utilisé ?      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df   = pd.read_csv(DEV_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Dev size:\", dev_df.shape)\n",
    "print(\"Test size:\", test_df.shape)\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428837e",
   "metadata": {},
   "source": [
    "## Load Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e58fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary size: 9104\n",
      "Embedding dimension: 100\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(W2V_MODEL_PATH)\n",
    "w2v = w2v_model.wv\n",
    "\n",
    "print(\"Word2Vec vocabulary size:\", len(w2v))\n",
    "print(\"Embedding dimension:\", w2v.vector_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61a453",
   "metadata": {},
   "source": [
    "## Build vocabulary from NER data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5c91c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (NER train): 2599\n",
      "Covered by Word2Vec: 2599\n",
      "OOV words: 0\n",
      "Coverage ratio: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def get_vocab_from_df(df):\n",
    "    vocab = set()\n",
    "    for sent in df[\"review\"]:\n",
    "        for w in sent.split():\n",
    "            vocab.add(w)\n",
    "    return vocab\n",
    "\n",
    "train_vocab = get_vocab_from_df(train_df)\n",
    "covered = [w for w in train_vocab if w in w2v]\n",
    "oov = [w for w in train_vocab if w not in w2v]\n",
    "\n",
    "print(f\"Vocabulary size (NER train): {len(train_vocab)}\")\n",
    "print(f\"Covered by Word2Vec: {len(covered)}\")\n",
    "print(f\"OOV words: {len(oov)}\")\n",
    "print(f\"Coverage ratio: {len(covered) / len(train_vocab):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e3f8e",
   "metadata": {},
   "source": [
    "## Build embedding matrix (Word2Vec → LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baba1283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words to 'patient':\n",
      "  cette           0.999\n",
      "  Le              0.999\n",
      "  produit         0.999\n",
      "  plus            0.999\n",
      "  qui             0.999\n",
      "\n",
      "Most similar words to 'traitement':\n",
      "  que             0.995\n",
      "  devra           0.995\n",
      "  médecin         0.995\n",
      "  TYSABRI         0.994\n",
      "  qu              0.993\n",
      "\n",
      "Most similar words to 'maladie':\n",
      "  évolution       0.998\n",
      "  du              0.998\n",
      "  administration  0.998\n",
      "  souris          0.998\n",
      "  la              0.998\n",
      "\n",
      "Most similar words to 'solution':\n",
      "  poudre          0.998\n",
      "  contient        0.997\n",
      "  Chaque          0.997\n",
      "  diluer          0.996\n",
      "  flacon          0.996\n"
     ]
    }
   ],
   "source": [
    "medical_words = [\"patient\", \"traitement\", \"maladie\", \"solution\"]\n",
    "\n",
    "for word in medical_words:\n",
    "    if word in w2v:\n",
    "        print(f\"\\nMost similar words to '{word}':\")\n",
    "        for w, s in w2v.most_similar(word, topn=5):\n",
    "            print(f\"  {w:15s} {s:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n'{word}' not in vocabulary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60a37f",
   "metadata": {},
   "source": [
    "## Save embedding matrix for LSTM script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b433e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix saved successfully.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"../../embeddings/ner\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "np.save(\n",
    "    os.path.join(OUTPUT_DIR, \"word2vec_medical_lstm_embeddings.npy\"),\n",
    "    embedding_matrix\n",
    ")\n",
    "\n",
    "print(\"Embedding matrix saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff1608",
   "metadata": {},
   "source": [
    "## Vérification des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1300e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train exists: True\n",
      "Dev exists: True\n",
      "Test exists: True\n",
      "Script exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "TRAIN = \"../../data/ner_processed/final/emea_train.csv\"\n",
    "DEV   = \"../../data/ner_processed/final/emea_dev.csv\"\n",
    "TEST  = \"../../data/ner_processed/final/emea_test.csv\"\n",
    "\n",
    "SCRIPT = \"../../scripts/cnn_classification.py\"\n",
    "\n",
    "print(\"Train exists:\", os.path.exists(TRAIN))\n",
    "print(\"Dev exists:\", os.path.exists(DEV))\n",
    "print(\"Test exists:\", os.path.exists(TEST))\n",
    "print(\"Script exists:\", os.path.exists(SCRIPT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93941f46",
   "metadata": {},
   "source": [
    "## Lancer Medical LSTM avec embeddings aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7f07571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files...\n",
      "Merging files...\n",
      "Building vocab...\n",
      "Encoding reviews...\n",
      "100%|█████████████████████████████████████| 706/706 [00:00<00:00, 435531.49it/s]\n",
      "100%|█████████████████████████████████████| 578/578 [00:00<00:00, 553621.31it/s]\n",
      "100%|█████████████████████████████████████| 649/649 [00:00<00:00, 538177.80it/s]\n",
      "[OK] Vocabulary saved to ../../data/ner_processed/final/emea_train.csv_vocab.pkl\n",
      "Vocabulary size: 4590\n",
      "Feature Shapes:\n",
      "===============\n",
      "Train set: (706, 128)\n",
      "Validation set: (578, 128)\n",
      "Test set: (649, 128)\n",
      "Taille vocabulaire 4590\n",
      "/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\n",
      "SentimentModelLSTM(\n",
      "  (embedding): Embedding(4590, 100)\n",
      "  (lstm): LSTM(100, 128, batch_first=True, dropout=0.25)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch 1/25 | Train Loss: 0.654 Train Acc: 0.787 | Val Loss: 0.614 Val Acc: 0.893\n",
      "Epoch 2/25 | Train Loss: 0.584 Train Acc: 0.902 | Val Loss: 0.548 Val Acc: 0.891\n",
      "Epoch 3/25 | Train Loss: 0.520 Train Acc: 0.898 | Val Loss: 0.492 Val Acc: 0.891\n",
      "Epoch 4/25 | Train Loss: 0.469 Train Acc: 0.897 | Val Loss: 0.445 Val Acc: 0.891\n",
      "Epoch 5/25 | Train Loss: 0.417 Train Acc: 0.905 | Val Loss: 0.409 Val Acc: 0.891\n",
      "Epoch 6/25 | Train Loss: 0.390 Train Acc: 0.899 | Val Loss: 0.391 Val Acc: 0.884\n",
      "Epoch 7/25 | Train Loss: 0.364 Train Acc: 0.900 | Val Loss: 0.365 Val Acc: 0.890\n",
      "Epoch 8/25 | Train Loss: 0.346 Train Acc: 0.900 | Val Loss: 0.352 Val Acc: 0.891\n",
      "Epoch 9/25 | Train Loss: 0.341 Train Acc: 0.897 | Val Loss: 0.357 Val Acc: 0.885\n",
      "[WARNING] Validation loss did not improved (0.352 --> 0.357)                    \n",
      "Epoch 10/25 | Train Loss: 0.326 Train Acc: 0.902 | Val Loss: 0.350 Val Acc: 0.888\n",
      "Epoch 11/25 | Train Loss: 0.329 Train Acc: 0.899 | Val Loss: 0.344 Val Acc: 0.891\n",
      "Epoch 12/25 | Train Loss: 0.326 Train Acc: 0.900 | Val Loss: 0.344 Val Acc: 0.891\n",
      "[WARNING] Validation loss did not improved (0.344 --> 0.344)                    \n",
      "Epoch 13/25 | Train Loss: 0.326 Train Acc: 0.902 | Val Loss: 0.341 Val Acc: 0.893\n",
      "Epoch 14/25 | Train Loss: 0.320 Train Acc: 0.904 | Val Loss: 0.328 Val Acc: 0.899\n",
      "Epoch 15/25 | Train Loss: 0.338 Train Acc: 0.897 | Val Loss: 0.345 Val Acc: 0.891\n",
      "[WARNING] Validation loss did not improved (0.328 --> 0.345)                    \n",
      "Epoch 16/25 | Train Loss: 0.343 Train Acc: 0.892 | Val Loss: 0.351 Val Acc: 0.888\n",
      "[WARNING] Validation loss did not improved (0.328 --> 0.351)                    \n",
      "Epoch 17/25 | Train Loss: 0.334 Train Acc: 0.899 | Val Loss: 0.344 Val Acc: 0.891\n",
      "[WARNING] Validation loss did not improved (0.328 --> 0.344)                    \n",
      "Epoch 18/25 | Train Loss: 0.329 Train Acc: 0.899 | Val Loss: 0.347 Val Acc: 0.890\n",
      "[WARNING] Validation loss did not improved (0.328 --> 0.347)                    \n",
      "Epoch 19/25 | Train Loss: 0.317 Train Acc: 0.904 | Val Loss: 0.351 Val Acc: 0.888\n",
      "[WARNING] Validation loss did not improved (0.328 --> 0.351)                    \n",
      "Early stopped at Epoch-19                                                       \n",
      "Training:  72%|▋| 18/25 [00:16<00:06,  1.12it/s, Val Loss: 0.351 | Val Acc: 0.88\n",
      "Inference: 100%|██████████████████████████████████| 6/6 [00:00<00:00, 14.96it/s]\n",
      "Accuracy: 0.8711, Loss: 0.3881\n",
      "Inference: 100%|██████████████████████████████████| 6/6 [00:00<00:00, 15.65it/s]\n",
      "\n",
      "========== TEST RESULTS ==========\n",
      "Model      : LSTM\n",
      "Precision  : 0.8475\n",
      "Recall     : 1.0000\n",
      "F1-score   : 0.9174\n",
      "Accuracy   : 0.8475\n",
      "\n",
      "Detailed classification report:\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        99\n",
      "           1       0.85      1.00      0.92       550\n",
      "\n",
      "    accuracy                           0.85       649\n",
      "   macro avg       0.42      0.50      0.46       649\n",
      "weighted avg       0.72      0.85      0.78       649\n",
      "\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.85      0.92       649\n",
      "\n",
      "    accuracy                           0.85       649\n",
      "   macro avg       0.50      0.42      0.46       649\n",
      "weighted avg       1.00      0.85      0.92       649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/cnn_classification.py \\\n",
    "    --model lstm \\\n",
    "    --train ../../data/ner_processed/final/emea_train.csv \\\n",
    "    --valid ../../data/ner_processed/final/emea_dev.csv \\\n",
    "    --test ../../data/ner_processed/final/emea_test.csv \\\n",
    "    --epochs 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0cc63c",
   "metadata": {},
   "source": [
    "## Press LSMT avec embeddings Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09522e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files...\n",
      "Merging files...\n",
      "Building vocab...\n",
      "Encoding reviews...\n",
      "100%|█████████████████████████████████| 35723/35723 [00:00<00:00, 232914.38it/s]\n",
      "100%|███████████████████████████████████| 2880/2880 [00:00<00:00, 271469.89it/s]\n",
      "100%|███████████████████████████████████| 2825/2825 [00:00<00:00, 277368.59it/s]\n",
      "[OK] Vocabulary saved to ../../data/ner_processed/final/press_train_final.csv_vocab.pkl\n",
      "Vocabulary size: 32002\n",
      "Feature Shapes:\n",
      "===============\n",
      "Train set: (35723, 128)\n",
      "Validation set: (2880, 128)\n",
      "Test set: (2825, 128)\n",
      "Taille vocabulaire 32002\n",
      "/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\n",
      "SentimentModelLSTM(\n",
      "  (embedding): Embedding(32002, 100)\n",
      "  (lstm): LSTM(100, 128, batch_first=True, dropout=0.25)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch 1/25 | Train Loss: 0.580 Train Acc: 0.732 | Val Loss: 0.801 Val Acc: 0.537\n",
      "Epoch 2/25 | Train Loss: 0.573 Train Acc: 0.745 | Val Loss: 0.790 Val Acc: 0.541\n",
      "Epoch 3/25 | Train Loss: 0.571 Train Acc: 0.746 | Val Loss: 0.801 Val Acc: 0.540\n",
      "[WARNING] Validation loss did not improved (0.790 --> 0.801)                    \n",
      "Epoch 4/25 | Train Loss: 0.571 Train Acc: 0.745 | Val Loss: 0.773 Val Acc: 0.540\n",
      "Epoch 5/25 | Train Loss: 0.566 Train Acc: 0.746 | Val Loss: 0.789 Val Acc: 0.540\n",
      "[WARNING] Validation loss did not improved (0.773 --> 0.789)                    \n",
      "Epoch 6/25 | Train Loss: 0.561 Train Acc: 0.745 | Val Loss: 0.713 Val Acc: 0.541\n",
      "Epoch 7/25 | Train Loss: 0.541 Train Acc: 0.746 | Val Loss: 0.732 Val Acc: 0.543\n",
      "[WARNING] Validation loss did not improved (0.713 --> 0.732)                    \n",
      "Epoch 8/25 | Train Loss: 0.534 Train Acc: 0.745 | Val Loss: 0.673 Val Acc: 0.541\n",
      "Epoch 9/25 | Train Loss: 0.523 Train Acc: 0.745 | Val Loss: 0.659 Val Acc: 0.541\n",
      "Epoch 10/25 | Train Loss: 0.487 Train Acc: 0.752 | Val Loss: 0.634 Val Acc: 0.674\n",
      "Epoch 11/25 | Train Loss: 0.445 Train Acc: 0.785 | Val Loss: 0.591 Val Acc: 0.692\n",
      "Epoch 12/25 | Train Loss: 0.416 Train Acc: 0.803 | Val Loss: 0.581 Val Acc: 0.698\n",
      "Epoch 13/25 | Train Loss: 0.390 Train Acc: 0.818 | Val Loss: 0.578 Val Acc: 0.707\n",
      "Epoch 14/25 | Train Loss: 0.366 Train Acc: 0.831 | Val Loss: 0.561 Val Acc: 0.693\n",
      "Epoch 15/25 | Train Loss: 0.350 Train Acc: 0.841 | Val Loss: 0.578 Val Acc: 0.711\n",
      "[WARNING] Validation loss did not improved (0.561 --> 0.578)                    \n",
      "Epoch 16/25 | Train Loss: 0.332 Train Acc: 0.851 | Val Loss: 0.563 Val Acc: 0.710\n",
      "[WARNING] Validation loss did not improved (0.561 --> 0.563)                    \n",
      "Epoch 17/25 | Train Loss: 0.317 Train Acc: 0.859 | Val Loss: 0.554 Val Acc: 0.712\n",
      "Epoch 18/25 | Train Loss: 0.302 Train Acc: 0.867 | Val Loss: 0.606 Val Acc: 0.717\n",
      "[WARNING] Validation loss did not improved (0.554 --> 0.606)                    \n",
      "Epoch 19/25 | Train Loss: 0.288 Train Acc: 0.875 | Val Loss: 0.540 Val Acc: 0.728\n",
      "Epoch 20/25 | Train Loss: 0.274 Train Acc: 0.880 | Val Loss: 0.544 Val Acc: 0.732\n",
      "[WARNING] Validation loss did not improved (0.540 --> 0.544)                    \n",
      "Epoch 21/25 | Train Loss: 0.258 Train Acc: 0.889 | Val Loss: 0.543 Val Acc: 0.739\n",
      "[WARNING] Validation loss did not improved (0.540 --> 0.543)       \n",
      "Epoch 22/25 | Train Loss: 0.250 Train Acc: 0.896 | Val Loss: 0.565 Val Acc: 0.742\n",
      "[WARNING] Validation loss did not improved (0.540 --> 0.565)       \n",
      "Epoch 23/25 | Train Loss: 0.235 Train Acc: 0.902 | Val Loss: 0.568 Val Acc: 0.740\n",
      "[WARNING] Validation loss did not improved (0.540 --> 0.568)       \n",
      "Epoch 24/25 | Train Loss: 0.222 Train Acc: 0.909 | Val Loss: 0.582 Val Acc: 0.744\n",
      "[WARNING] Validation loss did not improved (0.540 --> 0.582)       \n",
      "Early stopped at Epoch-24                                          \n",
      "Training:  92%|▉| 23/25 [17:09<01:29, 44.77s/it, Val Loss: 0.582 | Val Acc: 0.74\n",
      "Inference: 100%|████████████████████████████████| 23/23 [00:01<00:00, 16.94it/s]\n",
      "Accuracy: 0.8322, Loss: 0.3998\n",
      "Inference: 100%|████████████████████████████████| 23/23 [00:01<00:00, 16.87it/s]\n",
      "\n",
      "========== TEST RESULTS ==========\n",
      "Model      : LSTM\n",
      "Precision  : 0.8755\n",
      "Recall     : 0.8873\n",
      "F1-score   : 0.8814\n",
      "Accuracy   : 0.8251\n",
      "\n",
      "Detailed classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67       757\n",
      "           1       0.88      0.89      0.88      2068\n",
      "\n",
      "    accuracy                           0.83      2825\n",
      "   macro avg       0.78      0.77      0.77      2825\n",
      "weighted avg       0.82      0.83      0.82      2825\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       729\n",
      "           1       0.89      0.88      0.88      2096\n",
      "\n",
      "    accuracy                           0.83      2825\n",
      "   macro avg       0.77      0.78      0.77      2825\n",
      "weighted avg       0.83      0.83      0.83      2825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/cnn_classification.py \\\n",
    "    --model lstm \\\n",
    "    --train ../../data/ner_processed/final/press_train_final.csv \\\n",
    "    --valid ../../data/ner_processed/final/press_dev_final.csv \\\n",
    "    --test ../../data/ner_processed/final/press_test_final.csv \\\n",
    "    --epochs 25\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
