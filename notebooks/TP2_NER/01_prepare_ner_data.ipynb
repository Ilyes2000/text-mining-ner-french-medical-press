{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88d0958",
   "metadata": {},
   "source": [
    "# TP2 – Preparing NER Data\n",
    " \n",
    "**Lab:** TP2 Named Entity Recognition  \n",
    "**Notebook:** 01 Data Preparation  \n",
    "\n",
    "This notebook prepares the medical NER datasets for the CNN/LSTM and Transformer\n",
    "models provided by the instructor.\n",
    "\n",
    "The original datasets are in CONLL-like format and will be converted into\n",
    "CSV files compatible with the provided scripts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3997f0",
   "metadata": {},
   "source": [
    "## 1. Imports and Dataset Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef784db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81dc9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train exists: True\n",
      "Dev exists: True\n",
      "Test exists: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_DATA_DIR = \"../../data/ner_corpus/FrenchMed\"\n",
    "\n",
    "# EMEA corpus \n",
    "EMEA_TRAIN = os.path.join(BASE_DATA_DIR, \"EMEA/EMEAtrain_layer1_ID.conll\")\n",
    "EMEA_DEV   = os.path.join(BASE_DATA_DIR, \"EMEA/EMEAdev_layer1_ID.conll\")\n",
    "EMEA_TEST  = os.path.join(BASE_DATA_DIR, \"EMEA/EMEAtest_layer1_ID.conll\")\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"../../data/ner_processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Train exists:\", os.path.exists(EMEA_TRAIN))\n",
    "print(\"Dev exists:\", os.path.exists(EMEA_DEV))\n",
    "print(\"Test exists:\", os.path.exists(EMEA_TEST))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f937cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train exists: True\n",
      "Dev exists: True\n",
      "Test exists: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Train exists:\", os.path.exists(EMEA_TRAIN))\n",
    "print(\"Dev exists:\", os.path.exists(EMEA_DEV))\n",
    "print(\"Test exists:\", os.path.exists(EMEA_TEST))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f9f82",
   "metadata": {},
   "source": [
    "## Reading CONLL Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fd8bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(filepath):\n",
    "    sentences = []\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line == \"\":\n",
    "                if current_tokens:\n",
    "                    sentences.append((current_tokens, current_labels))\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                token = parts[1]\n",
    "                label = parts[-1]\n",
    "                current_tokens.append(token)\n",
    "                current_labels.append(label)\n",
    "\n",
    "        # Last sentence\n",
    "        if current_tokens:\n",
    "            sentences.append((current_tokens, current_labels))\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93012b3a",
   "metadata": {},
   "source": [
    "## Converting Sentences to a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dcfa64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_dataframe(sentences):\n",
    "    data = []\n",
    "    for tokens, labels in sentences:\n",
    "        data.append({\n",
    "            \"review\": \" \".join(tokens),\n",
    "            \"label\": labels\n",
    "        })\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ceada",
   "metadata": {},
   "source": [
    "## Processing One Dataset Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6592bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(conll_path, output_csv):\n",
    "    sentences = read_conll_file(conll_path)\n",
    "    df = sentences_to_dataframe(sentences)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved {len(df)} sentences to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78572c40",
   "metadata": {},
   "source": [
    "##  Generating CSV Files for NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3429c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 706 sentences to ../../data/ner_processed/emea_train.csv\n",
      "Saved 649 sentences to ../../data/ner_processed/emea_dev.csv\n",
      "Saved 578 sentences to ../../data/ner_processed/emea_test.csv\n"
     ]
    }
   ],
   "source": [
    "process_and_save(\n",
    "    EMEA_TRAIN,\n",
    "    os.path.join(OUTPUT_DIR, \"emea_train.csv\")\n",
    ")\n",
    "\n",
    "process_and_save(\n",
    "    EMEA_DEV,\n",
    "    os.path.join(OUTPUT_DIR, \"emea_dev.csv\")\n",
    ")\n",
    "\n",
    "process_and_save(\n",
    "    EMEA_TEST,\n",
    "    os.path.join(OUTPUT_DIR, \"emea_test.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d7dd8",
   "metadata": {},
   "source": [
    "## Inspecting the Prepared Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6de59b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRIALT</td>\n",
       "      <td>['B-CHEM']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EMEA / H / C / 551</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qu ’ est ce que Prialt ?</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'B-CHEM', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prialt est une solution pour perfusion contena...</td>\n",
       "      <td>['B-CHEM', 'O', 'O', 'B-CHEM', 'O', 'B-PROC', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dans quel cas Prialt est - il utilisé ?</td>\n",
       "      <td>['O', 'O', 'O', 'B-CHEM', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0                                             PRIALT   \n",
       "1                                 EMEA / H / C / 551   \n",
       "2                           Qu ’ est ce que Prialt ?   \n",
       "3  Prialt est une solution pour perfusion contena...   \n",
       "4            Dans quel cas Prialt est - il utilisé ?   \n",
       "\n",
       "                                               label  \n",
       "0                                         ['B-CHEM']  \n",
       "1                ['O', 'O', 'O', 'O', 'O', 'O', 'O']  \n",
       "2           ['O', 'O', 'O', 'O', 'O', 'B-CHEM', 'O']  \n",
       "3  ['B-CHEM', 'O', 'O', 'B-CHEM', 'O', 'B-PROC', ...  \n",
       "4  ['O', 'O', 'O', 'B-CHEM', 'O', 'O', 'O', 'O', ...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(OUTPUT_DIR, \"emea_train.csv\"))\n",
    "dev_df   = pd.read_csv(os.path.join(OUTPUT_DIR, \"emea_dev.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(OUTPUT_DIR, \"emea_test.csv\"))\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9be2f",
   "metadata": {},
   "source": [
    "## Sanity Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6777ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 706\n",
      "\n",
      "Example sentence:\n",
      "PRIALT\n",
      "\n",
      "NER labels:\n",
      "['B-CHEM']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training sentences:\", len(train_df))\n",
    "\n",
    "example_idx = 0\n",
    "print(\"\\nExample sentence:\")\n",
    "print(train_df.iloc[example_idx][\"review\"])\n",
    "print(\"\\nNER labels:\")\n",
    "print(train_df.iloc[example_idx][\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e7a47",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we:\n",
    "- Loaded the medical NER corpus in CONLL format\n",
    "- Grouped tokens into sentences\n",
    "- Preserved BIO NER labels\n",
    "- Exported clean CSV files compatible with:\n",
    "  - cnn_classification.py\n",
    "  - transformer.py\n",
    "\n",
    "These processed datasets will be used directly in the next notebooks\n",
    "without modifying the instructor-provided scripts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb9c92",
   "metadata": {},
   "source": [
    "## Same with press DATASET: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6c57440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PRESS_DIR = \"../../data/ner_corpus/FrenchPress\"\n",
    "\n",
    "PRESS_TRAIN = os.path.join(BASE_PRESS_DIR, \"fra4_ID.train\")\n",
    "PRESS_DEV   = os.path.join(BASE_PRESS_DIR, \"fra4_ID.dev\")\n",
    "PRESS_TEST  = os.path.join(BASE_PRESS_DIR, \"fra4_ID.test\")\n",
    "\n",
    "OUTPUT_DIR = \"../../data/ner_processed/final\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9878b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_conll_press(path):\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:  # end of sentence\n",
    "                if tokens:\n",
    "                    sentences.append({\n",
    "                        \"review\": \" \".join(tokens),\n",
    "                        \"label\": labels\n",
    "                    })\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "\n",
    "            token = parts[1]\n",
    "            ner = parts[-1].upper()  # normalize\n",
    "\n",
    "            tokens.append(token)\n",
    "            labels.append(ner)\n",
    "\n",
    "    if tokens:\n",
    "        sentences.append({\n",
    "            \"review\": \" \".join(tokens),\n",
    "            \"label\": labels\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223a2ff",
   "metadata": {},
   "source": [
    "## Convert sentences to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a7d621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 35723 sentences saved → ../../data/ner_processed/final/press_train.csv\n",
      "dev: 2825 sentences saved → ../../data/ner_processed/final/press_dev.csv\n",
      "test: 2880 sentences saved → ../../data/ner_processed/final/press_test.csv\n"
     ]
    }
   ],
   "source": [
    "BASE = \"../../data/ner_corpus/FrenchPress\"\n",
    "OUT  = \"../../data/ner_processed/final\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "splits = {\n",
    "    \"train\": \"fra4_ID.train\",\n",
    "    \"dev\":   \"fra4_ID.dev\",\n",
    "    \"test\":  \"fra4_ID.test\"\n",
    "}\n",
    "\n",
    "for split, file in splits.items():\n",
    "    path = os.path.join(BASE, file)\n",
    "    df = read_conll_press(path)\n",
    "    out = os.path.join(OUT, f\"press_{split}.csv\")\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"{split}: {len(df)} sentences saved → {out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "370d2365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Patricia', 'Martin', ',', 'que', 'voici', ',', 'que', 'voilà', '!', 'oh', ',', 'bonjour', 'Nicolas', 'Stoufflet', '.']\n",
      "['B-PERS', 'I-PERS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERS', 'I-PERS', 'O']\n",
      "15 15\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/ner_processed/final/press_train.csv\")\n",
    "df.head()\n",
    "\n",
    "# Vérifier alignement\n",
    "i = 0\n",
    "print(df.iloc[i][\"review\"].split())\n",
    "print(df.iloc[i][\"label\"])\n",
    "print(len(df.iloc[i][\"review\"].split()), len(eval(df.iloc[i][\"label\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f357ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press train exists: True\n",
      "Press dev exists: True\n",
      "Press test exists: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Press train exists:\", os.path.exists(\"../../data/ner_processed/final/press_train.csv\"))\n",
    "print(\"Press dev exists:\", os.path.exists(\"../../data/ner_processed/final/press_dev.csv\"))\n",
    "print(\"Press test exists:\", os.path.exists(\"../../data/ner_processed/final/press_test.csv\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
